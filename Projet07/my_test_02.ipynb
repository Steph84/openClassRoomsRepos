{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "329bfbe0-3b60-48dc-b9a1-01ae1c98a34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/slerendu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1057a785-d9ee-48c1-bba6-699b787ccb75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a208de2e-63ab-4864-9554-4c7c2d4f4fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b76356e-d84d-4f49-befe-765bb901e409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/slerendu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk import pos_tag, tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1bedf16-bb3b-48a0-a2d0-3bfc90d33da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/training.1600000.processed.noemoticon.csv\", sep=',',\n",
    "                   encoding=\"ISO-8859-1\", names=[\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6131634-42dd-4aa5-846b-8de8308c291a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bab141f1-1247-4ef1-9d2f-faa63f3b109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9fa9a04-c99f-43bc-80e1-0d33dcfa7517",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_file_name = \"models/mod_bernoulli.h5\"\n",
    "loaded_model = pickle.load(open(model_file_name, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ddad0af-cb57-48e5-ad48-c62f40d9c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name_2 = \"models/count_vectorizer.h5\"\n",
    "count_vectorizer = pickle.load(open(model_file_name_2, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273b5aff-4c82-4f73-ae8e-d9750584efff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### not hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "714dbf5d-757f-4170-aede-714934a46d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_textism(sentence):\n",
    "    neo_sentence = []\n",
    "    for word in sentence:\n",
    "        if word == 'u':\n",
    "            neo_sentence.append('you')\n",
    "        elif word == 'r':\n",
    "            neo_sentence.append('are')\n",
    "        elif word == 'ur':\n",
    "            neo_sentence.append('your')\n",
    "        elif word == 'some1':\n",
    "            neo_sentence.append('someone')\n",
    "        elif word == 'yrs':\n",
    "            neo_sentence.append('years')\n",
    "        elif word == 'hrs':\n",
    "            neo_sentence.append('hours')\n",
    "        elif word == 'mins':\n",
    "            neo_sentence.append('minutes')\n",
    "        elif word == 'secs':\n",
    "            neo_sentence.append('seconds')\n",
    "        elif word == 'pls' or word == 'plz':\n",
    "            neo_sentence.append('please')\n",
    "        elif word == '2morow':\n",
    "            neo_sentence.append('tomorrow')\n",
    "        elif word == '2day':\n",
    "            neo_sentence.append('today')\n",
    "        elif word == '2nite':\n",
    "            neo_sentence.append('tonight')\n",
    "        elif word == '4got' or word == '4gotten':\n",
    "            neo_sentence.append('forget')\n",
    "        elif word == 'amp' or word == 'quot' or word == 'lt' or word == 'gt' or word == 'Â½25':\n",
    "            neo_sentence.append('')\n",
    "        else:\n",
    "            neo_sentence.append(word)\n",
    "    return neo_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07f3e4e4-3bec-4708-b278-4c1cd94f4523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Return the corresponding character for a word use in the lemmatization\n",
    "    \n",
    "    Parameters:\n",
    "    word (str): a word\n",
    "    \n",
    "    Returns:\n",
    "    str: the corresponding character\n",
    "    \"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b90c3ca-5aa1-45ef-8caa-58d24e3c966b",
   "metadata": {},
   "source": [
    "### Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55d868e5-1f1b-4727-af66-a06b8e7a0093",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_neg = '@ home studying for maths wooot ! im so going to fail this shit '\n",
    "phrase_pos = '@ashlili LIKE MEEEEEEEEE '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dcb37a67-ad6e-4da5-9af6-a18eff64c5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = phrase_neg.split()\n",
    "X_wo_arobas = [x for x in X if not x.startswith(\"@\")]\n",
    "X_new = [x for x in X_wo_arobas if not x.startswith(\"http\")]\n",
    "res = ' '.join(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "34939091-fb22-479a-87f5-09abaefe7317",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_exclude = [\n",
    "    \"...\", \"..\"\n",
    "]\n",
    "tags_to_remove=[\"NNP\", \"VBG\", \"VBN\", \"CD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e41030c7-0942-475d-9fbe-d90b63140386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building stopwords list\n",
    "stopW = stopwords.words('english')\n",
    "stopW.extend(string.punctuation)\n",
    "\n",
    "temp_res = \"\"\n",
    "# normalisation\n",
    "temp_res = res.lower()\n",
    "# tokenization\n",
    "tk = tokenize.TweetTokenizer(reduce_len=True)\n",
    "temp_res = tk.tokenize(temp_res)\n",
    "\n",
    "# clean the sms language to usefull langage\n",
    "temp_res = clean_textism(temp_res)\n",
    "\n",
    "# remove stopwords\n",
    "temp_res = [word for word in temp_res if word not in stopW]\n",
    "\n",
    "# lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "temp_res = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in temp_res]\n",
    "\n",
    "temp_res = pos_tag(temp_res)\n",
    "temp_res = [x[0] for x in temp_res if x[1] not in tags_to_remove]\n",
    "\n",
    "temp_res = [x for x in temp_res if x not in words_to_exclude]\n",
    "\n",
    "res = ' '.join([x for x in temp_res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52a3987f-5309-448b-a340-8fa4dace1457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'home study math wooot im go fail shit'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "54c13355-d2ad-4202-a1ce-14141a5d4b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embed = count_vectorizer.transform([res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ecf5b6d3-1749-40d0-bc84-47ca8e0fa03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x800 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 7 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c76c0968-971d-4fae-8927-27283ebfa4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_res = loaded_model.predict(X_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b2db8681-e040-4841-b9fa-2403f33b1929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc4da8e2-54d1-4e27-b301-03952ca29c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small Bam......\n"
     ]
    }
   ],
   "source": [
    "if temp_res[0] > 0.5:\n",
    "    print(\"Double Bam !!\")\n",
    "else:\n",
    "    print(\"Small Bam......\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c556b26-fc0f-4368-9804-83831f6b909e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env_prj07_04",
   "language": "python",
   "name": "conda_env_prj07_04"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
