{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd238a40-9d8c-4d6f-ac02-fb9082ba83d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485e8be7-23ed-4780-8aac-8001b09df5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8ddae5-db2a-47aa-a542-546706c712a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/slerendu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import time\n",
    "\n",
    "# pour traitement du language (NLP)\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk import tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from sklearn import metrics as metrics_sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6554ce4-caca-443f-9400-f2759cd12366",
   "metadata": {
    "tags": []
   },
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07769ada-7d71-40f8-a536-d085af2b40e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-06 11:54:33.846863: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-06 11:54:34.167306: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-06 11:54:35.023474: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8089494032435413241\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4370071552\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 14858978808541407720\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1660 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-06 11:54:35.950376: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-06 11:54:35.966001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-06 11:54:35.966373: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-06 11:54:36.055259: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-06 11:54:36.055392: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-06 11:54:36.055458: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-06 11:54:36.055526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /device:GPU:0 with 4167 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be53c0a1-6e04-49f8-a509-739379837666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-06 11:54:36.060254: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-06 11:54:36.060398: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-06 11:54:36.060463: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ff5eec-0c40-4893-89dc-d34aee008003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU name:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print('GPU name: ', tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199cc229-1f24-4614-b7ab-cb19d31226a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://www.tensorflow.org/guide/gpu\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7db231-f17f-48e3-93bb-a7c0e8fe2f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac99e5-6a0a-4160-b842-fca86fda99da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dae96e44-c262-4c86-b9c6-809de93e0fbd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Méthodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5febac41-2e10-4ebf-afaa-0020fd9808b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Méthodes pour les datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5065e53d-91a3-4ca1-87e4-5f4b0085b718",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pour les pie charts, pour afficher les pourcentages et les libellés uniquement avec le taux > limit_perc%\n",
    "limit_perc = 5\n",
    "def show_perc(pct):\n",
    "    \"\"\" Affiche le pourcentage de chaque portion si la condition est vérifiée \"\"\"\n",
    "    return ('%.1f%%' % pct) if pct > limit_perc else ''\n",
    "\n",
    "def show_labels(data):\n",
    "    list = []\n",
    "    for val, cnt in data.items():\n",
    "        if (cnt*100/np.sum(data)) > limit_perc :\n",
    "            list.append(val)\n",
    "        else:\n",
    "            list.append('')\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349552c8-39e4-4400-8ae7-1885eacd2a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_unique_values(df, col, display_pie=True, silent=False):\n",
    "    \"\"\"\n",
    "    Affiche la liste des valeurs uniques contenues dans une colonne\n",
    "    Affiche le pie chart de ces données également\n",
    "    \"\"\"\n",
    "    temp_val_count = df[col].value_counts(normalize=True)\n",
    "    df_val_count = temp_val_count.rename_axis(\"label\").to_frame(\"%\")\n",
    "    df_val_count[\"%\"] = round(df_val_count[\"%\"]*100, 2)\n",
    "    df_val_count[\"%_cumul\"] = df_val_count[\"%\"].cumsum()\n",
    "    # afficher la majeur partie des catégories\n",
    "    sub_display = df_val_count.loc[df_val_count[\"%_cumul\"] <= 85, :]\n",
    "    \n",
    "    if silent:\n",
    "        return sub_display\n",
    "    \n",
    "    if len(sub_display) < 5:\n",
    "        display(df_val_count.head())\n",
    "    else:\n",
    "        display(sub_display.head(len(sub_display)))\n",
    "    if display_pie:\n",
    "        temp_val_count.plot(kind='pie', autopct=show_perc, labels=show_labels(temp_val_count), label='')\n",
    "        plt.axis('equal')\n",
    "        plt.show()\n",
    "    return sub_display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7102a8d-0820-426a-be51-096db8e403b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Méthodes pour preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba90338e-84d7-4e7c-bca2-ae6bc0ebfaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Return the corresponding character for a word use in the lemmatization\n",
    "    \n",
    "    Parameters:\n",
    "    word (str): a word\n",
    "    \n",
    "    Returns:\n",
    "    str: the corresponding character\n",
    "    \"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82505d9-2ace-4ed9-bf3d-2aa93772f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailled_lemmatizer(sentence):\n",
    "    \"\"\"Lemmatize a sentence and return it\n",
    "    \n",
    "    Parameters:\n",
    "    sentence (list(str)): a list of words\n",
    "    \n",
    "    Returns:\n",
    "    (list(str)): a list of lemmatized words\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    result = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in sentence]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554db83d-81ab-412e-8723-7d1c4d8f5e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(X):\n",
    "    X = X.split()\n",
    "    X_wo_arobas = [x for x in X if not x.startswith(\"@\")]\n",
    "    X_new = [x for x in X_wo_arobas if not x.startswith(\"http\")]\n",
    "    return ' '.join(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3f2b8e-aa82-4021-9a23-f0d35960a8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pos_tag(df, col, tags_to_remove=[], debug=False):\n",
    "    \"\"\"Filter words from a list of Part-Of-Speech tags\n",
    "    \n",
    "    Parameters:\n",
    "    df (dataframe): input dataframe\n",
    "    col (str): column to process\n",
    "    tags_to_remove list(str): a list of POS tag\n",
    "    debug (bool): show debug elements\n",
    "    \n",
    "    Returns:\n",
    "    list(list(str)): return a list of document. Each document is a list of words    \n",
    "    \"\"\"\n",
    "    result = []\n",
    "    if not tags_to_remove:\n",
    "        print(\"Aucun filtre n'a été défini.\")\n",
    "        return df[col]\n",
    "    for index, row in df.iterrows():\n",
    "        temp_res = pos_tag(row[col])\n",
    "        temp_res = [x for x in temp_res if x[1] not in tags_to_remove]\n",
    "        # if debug:\n",
    "        #     print(temp_res[0])\n",
    "        result.append([x[0] for x in temp_res])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6166a297-a740-40d8-ab7e-a02ffe94f060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_exclude_words(df, col, words_to_exclude=[], debug=False):\n",
    "    \"\"\"Filter words from a list of specific words\n",
    "    \n",
    "    Parameters:\n",
    "    df (dataframe): input dataframe\n",
    "    col (str): column to process\n",
    "    words_to_exclude list(str): a list of words\n",
    "    debug (bool): show debug elements\n",
    "    \n",
    "    Returns:\n",
    "    list(list(str)): return a list of document. Each document is a list of words    \n",
    "    \"\"\"\n",
    "    result = []\n",
    "    if not words_to_exclude:\n",
    "        print(\"Aucun mots à exclure.\")\n",
    "        return df[col]\n",
    "    for index, row in df.iterrows():\n",
    "        result.append([x for x in row[col] if x not in words_to_exclude])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e5976d-f627-4ef3-adcd-b4eebce2e431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_wordcloud(corpus, debug=False):\n",
    "    \"\"\"Display a WordCloud picture from a corpus\n",
    "    \n",
    "    Parameters:\n",
    "    corpus (dict): a Counter dictionary with the frequency of each words\n",
    "    debug (bool): show debug elements\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    \"\"\"\n",
    "    wordcloud = WordCloud(\n",
    "            random_state = 42,\n",
    "            normalize_plurals = False,\n",
    "            width = 600, \n",
    "            height= 300,\n",
    "            max_words = 100,\n",
    "            stopwords = [],\n",
    "            colormap=\"BrBG\")\n",
    "\n",
    "    wordcloud.generate_from_frequencies(corpus)\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1, figsize = (12,8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8da309e-61fb-4799-a138-1e6c8355452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_textism(sentence):\n",
    "    neo_sentence = []\n",
    "    for word in sentence:\n",
    "        if word == 'u':\n",
    "            neo_sentence.append('you')\n",
    "        elif word == 'r':\n",
    "            neo_sentence.append('are')\n",
    "        elif word == 'ur':\n",
    "            neo_sentence.append('your')\n",
    "        elif word == 'some1':\n",
    "            neo_sentence.append('someone')\n",
    "        elif word == 'yrs':\n",
    "            neo_sentence.append('years')\n",
    "        elif word == 'hrs':\n",
    "            neo_sentence.append('hours')\n",
    "        elif word == 'mins':\n",
    "            neo_sentence.append('minutes')\n",
    "        elif word == 'secs':\n",
    "            neo_sentence.append('seconds')\n",
    "        elif word == 'pls' or word == 'plz':\n",
    "            neo_sentence.append('please')\n",
    "        elif word == '2morow':\n",
    "            neo_sentence.append('tomorrow')\n",
    "        elif word == '2day':\n",
    "            neo_sentence.append('today')\n",
    "        elif word == '2nite':\n",
    "            neo_sentence.append('tonight')\n",
    "        elif word == '4got' or word == '4gotten':\n",
    "            neo_sentence.append('forget')\n",
    "        elif word == 'amp' or word == 'quot' or word == 'lt' or word == 'gt' or word == '½25':\n",
    "            neo_sentence.append('')\n",
    "        else:\n",
    "            neo_sentence.append(word)\n",
    "    return neo_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bb43d8-cea3-4cee-9a96-f3f9537a2e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_words(df, col, stem_or_lemma=\"stem\", debug=False):\n",
    "    \"\"\"Compute all the basics string tranformations in NLP\n",
    "    Normalisation, Tokenization, Remove of stopwords, Stemmation or Lemmatization\n",
    "    \n",
    "    Parameters:\n",
    "    df (dataframe): input dataframe\n",
    "    col (str): column to process\n",
    "    stem_or_lemma (str): choose between stemmation or lemmatization\n",
    "    debug (bool): show debug elements\n",
    "    \n",
    "    Returns:\n",
    "    list(list(str)): return a list of document. Each document is a list of words\n",
    "    \"\"\"\n",
    "    result = []    \n",
    "    # building stopwords list\n",
    "    stopW = stopwords.words('english')\n",
    "    stopW.extend(string.punctuation)\n",
    "    \n",
    "    for index, row in df.iterrows():        \n",
    "        temp_res = \"\"\n",
    "        # normalisation\n",
    "        temp_res = row[col].lower()\n",
    "        # tokenization\n",
    "        tk = tokenize.TweetTokenizer(reduce_len=True)\n",
    "        temp_res = tk.tokenize(temp_res)\n",
    "        \n",
    "        # clean the sms language to usefull langage\n",
    "        temp_res = clean_textism(temp_res)\n",
    "        \n",
    "        # remove stopwords\n",
    "        temp_res = [word for word in temp_res if word not in stopW]\n",
    "        \n",
    "        # stemmation or lemmatization\n",
    "        if stem_or_lemma == \"stem\":\n",
    "            stemmer = PorterStemmer()\n",
    "            temp_res = [stemmer.stem(elt) for elt in temp_res]\n",
    "        elif stem_or_lemma == \"lemma\":\n",
    "            temp_res = detailled_lemmatizer(temp_res)\n",
    "        else:\n",
    "            print(\"stem or lemma only\")\n",
    "            \n",
    "        result.append(temp_res)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d38d84f-f693-4dc1-b624-2d69c6e7c695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def natural_language_processing(\n",
    "    df, col, stem_or_lemma=\"stem\", tags_to_remove=[], words_to_exclude=[],\n",
    "    max_threshold=999, min_threshold=0, word_cloud=True, debug=False):\n",
    "    \"\"\"Compute NLP preprocessing methods\n",
    "    \n",
    "    Parameters:\n",
    "    df (dataframe): input dataframe\n",
    "    col (str): column to process\n",
    "    stem_or_lemma (str): choose between stemmation or lemmatization\n",
    "    tags_to_remove list(str): a list of POS tag\n",
    "    words_to_exclude list(str): a list of words\n",
    "    max_threshold (int): upper threshold to filter word frequency\n",
    "    min_threshold (int): lower threshold to filter word frequency\n",
    "    word_cloud (bool): display WordCloud representation\n",
    "    debug (bool): show debug elements\n",
    "    \n",
    "    Returns:\n",
    "    list(list(str)): return a list of document. Each document is a list of words    \n",
    "    \"\"\"\n",
    "    # df = input_df.copy()\n",
    "    # supprimer des mots spécifiques à twitter (commençant par @ et hhtp)\n",
    "    df[col] = df[col].apply(clean_text)\n",
    "    \n",
    "    # preprocessing part\n",
    "    preproc_res = preprocessing_words(df, col, stem_or_lemma=stem_or_lemma, debug=debug)\n",
    "    df.insert(0, 'preproc_text', preproc_res)\n",
    "    \n",
    "    # filter by pos tag part\n",
    "    filtpos_res = filter_pos_tag(df, \"preproc_text\", tags_to_remove=tags_to_remove, debug=debug)\n",
    "    df.insert(0, 'filtpos_text', filtpos_res)\n",
    "    \n",
    "    # filter by excluding words\n",
    "    filtexcl_words = filter_exclude_words(df, \"filtpos_text\", words_to_exclude=words_to_exclude, debug=debug)\n",
    "    df.insert(0, 'filtexcl_words', filtexcl_words)\n",
    "    \n",
    "    if debug:\n",
    "        display(df[[col, 'preproc_text', 'filtpos_text', 'filtexcl_words']].head())\n",
    "    \n",
    "    # concatenate all documents into a single corpus\n",
    "    corpus = df[\"filtexcl_words\"].tolist()\n",
    "    corpus = [item for sublist in corpus for item in sublist]\n",
    "    \n",
    "    word_counts = Counter(corpus)\n",
    "    print(\"Il y a un total de\", len(word_counts), \"mots différents dans tout le corpus.\")\n",
    "    word_counts_threshold = {x: count for x, count in word_counts.items() if count > min_threshold}\n",
    "    word_counts_threshold = {x: count for x, count in word_counts_threshold.items() if count < max_threshold}\n",
    "    print(\"Après filtrage, on garde les mots aparaissant plus de\", min_threshold,\n",
    "          \"fois et moins de\", max_threshold, \"fois. Il reste alors\",\n",
    "          len(word_counts_threshold), \"mots différents dans tout le corpus.\\n\")\n",
    "    word_counts = Counter(word_counts_threshold)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"------------ Top 20 plus communs : ------------\")\n",
    "        display_most_common = [(i, word_counts[i], word_counts[i] / len(corpus) * 100.0) for i, count in word_counts.most_common(20)]\n",
    "        for elt in display_most_common:\n",
    "            print(elt)\n",
    "        print(\"\\n------------ Top 20 moins communs : ------------\")\n",
    "        display_least_common = [(i, word_counts[i], word_counts[i] / len(corpus) * 100.0) for i, count in word_counts.most_common()[-20:]]\n",
    "        for elt in display_least_common:\n",
    "            print(elt)\n",
    "        \n",
    "    simplified_corpus = word_counts_threshold.keys()\n",
    "    \n",
    "    # display wordcloud part\n",
    "    if word_cloud:\n",
    "        display_wordcloud(word_counts_threshold, debug=debug)\n",
    "    \n",
    "    return simplified_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e76c55-ee49-4f2c-8a28-a170a44847d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Méthodes pour BOW and Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aff3b59-96e8-4770-8e9a-a4296e64c7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def representation_by_tf_idf(corpus):\n",
    "    \"\"\"Compute corpus into a tf-idf vectorisation\n",
    "    \n",
    "    Parameters:\n",
    "    corpus (list(list(str)): a list of documents\n",
    "    \n",
    "    Returns:\n",
    "    a matrix of TF-IDF features\n",
    "    the list of features names\n",
    "    \"\"\"\n",
    "    vect = TfidfVectorizer(ngram_range=(1, 1))\n",
    "    tfidf_mat = vect.fit_transform(corpus)\n",
    "    features_names = vect.get_feature_names_out()\n",
    "    return tfidf_mat, features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c6e71-6c4e-4c08-b26a-44fc5c695095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "def representation_by_doc2vec(corpus):\n",
    "    X = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus)]\n",
    "\n",
    "    # Train doc2vec model\n",
    "    doc2vec = Doc2Vec()\n",
    "    doc2vec.build_vocab(X)\n",
    "    doc2vec.train(X, total_examples=doc2vec.corpus_count, epochs=doc2vec.epochs)\n",
    "\n",
    "    # Vectorize text\n",
    "    X = [doc2vec.infer_vector(doc.words) for doc in X]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e68972-f0ca-4b49-b7ef-d691af97a1e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Méthodes pour les métriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60f33d5-0190-4fe1-83c4-585315c7dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_confusion_matrix(actual_class, predict_class, display_labels=None, specifity=False):\n",
    "    \"\"\"display the confusion matrix\n",
    "    \n",
    "    Parameters:\n",
    "    actual_class : serie of the actual classes\n",
    "    predict_class : serie of the predicted classes\n",
    "    display_labels list(str)) : list of labels to show in the confusion matrix\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"Calcul et affichage de la matrice de confusion\")\n",
    "    # pour calculer la matrice de confusion\n",
    "    cm = metrics_sklearn.confusion_matrix(actual_class, predict_class)#, labels=display_labels)\n",
    "    cm = cm.T\n",
    "    \n",
    "    # pour afficher la matrice de confusion\n",
    "    displ = metrics_sklearn.ConfusionMatrixDisplay(cm, display_labels=display_labels)\n",
    "    displ.plot()\n",
    "    displ.ax_.xaxis.tick_top()\n",
    "    displ.ax_.xaxis.set_label_position('top') \n",
    "    plt.xlabel('Actual Label')\n",
    "    plt.ylabel('Predicted Label')\n",
    "    plt.gcf().axes[0].tick_params()\n",
    "    plt.gcf().axes[1].tick_params()\n",
    "    plt.show()\n",
    "    \n",
    "    # compute specificity\n",
    "    if specifity:\n",
    "        speci = cm[0, 0]/(cm[0, 0] + cm[1, 0])\n",
    "        print(\"Spécificité :\", speci)\n",
    "        return speci\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caf04fb-27a2-428c-9518-251fc71be9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(model_name, y_test, y_test_pred):  \n",
    "    print(\"Calcul et affichage de la courbe ROC\")\n",
    "    [fpr_te, tpr_te, thr_te] = metrics_sklearn.roc_curve(y_test, y_test_pred)#, pos_label=1)\n",
    "    plt.plot(fpr_te, tpr_te, color='coral', lw=2, label=\"\")\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('1 - Specificity', fontsize=14)\n",
    "    plt.ylabel('Sensitivity', fontsize=14)\n",
    "    plt.title(\"ROC du classifier {}\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cc7ee9-787a-4608-b97a-44372819f4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_classification_metrics(estimator, y_test, y_test_pred, list_metrics=[\"accuracy\"], all_metrics=False, roc_curve=False, beta=1):\n",
    "    print(\"Calcul et affichage des différentes métriques\")\n",
    "    result = {}\n",
    "    if all_metrics:\n",
    "        list_metrics = [\"accuracy\", \"recall\", \"precision\", \"auc\", \"f1\", \"fbeta\"]\n",
    "    for met in list_metrics:\n",
    "        if met == \"accuracy\":\n",
    "            te_res = metrics_sklearn.accuracy_score(y_test, y_test_pred).round(3)\n",
    "            print(\"Accuracy :\", te_res)\n",
    "        elif met == \"precision\":     \n",
    "            te_res = metrics_sklearn.precision_score(y_test, y_test_pred, average='macro').round(3)\n",
    "            print(\"Précision :\", te_res)\n",
    "        elif met == \"recall\":  \n",
    "            te_res = metrics_sklearn.recall_score(y_test, y_test_pred, average='macro').round(3)\n",
    "            print(\"Recall :\", te_res)\n",
    "        elif met == \"auc\":    \n",
    "            te_res = metrics_sklearn.roc_auc_score(y_test, y_test_pred, average='macro').round(3)\n",
    "            print(\"AUC :\", te_res)\n",
    "            if roc_curve:\n",
    "                plot_roc_curve(type(estimator).__name__, y_test, y_test_pred)\n",
    "        elif met == \"f1\":   \n",
    "            te_res = metrics_sklearn.f1_score(y_test, y_test_pred, average='macro').round(3)\n",
    "            print(\"F1-score :\", te_res)\n",
    "        elif met == \"fbeta\":\n",
    "            te_res = metrics_sklearn.fbeta_score(y_test, y_test_pred, beta=beta, average='macro').round(3)\n",
    "            print(\"Fbeta-score (beta=\", beta,  \") :\", te_res)\n",
    "        else:\n",
    "            print(\"La métrique\", met, \"n'est pas dans la liste suivante accuracy, recall, precision, auc, f1 et fbeta. Veuillez vérifier la saisie.\")\n",
    "        result[met] = te_res\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7652813f-ec96-4506-914f-cddc46b1cf1e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Méthodes pour les modèles de régression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c4827d-6f4b-488a-9a8a-8ad686ff31d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "def get_naive_bayes_model(model_name=\"gaussian\"):\n",
    "    \"\"\"return an instance of the model selected\n",
    "    \n",
    "    Parameters:\n",
    "    model_name (str) : names of naive bayes model used\n",
    "    \n",
    "    Returns:\n",
    "    instance of the model\n",
    "    \"\"\"\n",
    "    if model_name == \"gaussian\":\n",
    "        return naive_bayes.GaussianNB()\n",
    "    elif model_name == \"multinomial\":\n",
    "        return naive_bayes.MultinomialNB()\n",
    "    elif model_name == \"bernoulli\":\n",
    "        return naive_bayes.BernoulliNB()\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463e279c-2b18-42a5-9563-e1fc1ecbe8f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Méthodes pour les Réseaux de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ba2797-958f-4818-80b1-a15cc016aa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text\n",
    "def tokenize_corpus(corpus):\n",
    "    tokenizer = text.Tokenizer()\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59410b0c-3be5-41d4-8cca-4bef9986db78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "def word_embedding_by_word2vec(embed_dim, data):\n",
    "    w2v_model = models.word2vec.Word2Vec(data, vector_size=embed_dim, workers=8)\n",
    "    # w2v_model.build_vocab(data)\n",
    "    w2v_model.train(data, total_examples=len(data), epochs=50)\n",
    "    return w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63716c57-4e6a-4f97-8269-90ba902c094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "# !unzip data/glove.twitter.27B.zip\n",
    "def word_embedding_by_glove(corpus):\n",
    "    GLOVE_EMB = 'glove.twitter.27B.200d.txt'\n",
    "      \n",
    "    # compute embeddings index from GloVe\n",
    "    embeddings_index = {}\n",
    "    f = open(GLOVE_EMB)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = value = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Found %s word vectors.' %len(embeddings_index))\n",
    "    return embeddings_index   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5925e51-e68c-49b3-8c36-b0961ea9acce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_corpus_to_word_embedding(corpus, model_name=\"word2vec\"):\n",
    "    tokenizer = tokenize_corpus(corpus)\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(tokenizer.word_index) + 1  \n",
    "    \n",
    "    if model_name == \"word2vec\":\n",
    "        embed_dim = 800\n",
    "        \n",
    "        # word embedding using Word2Vec\n",
    "        w2v_model = word_embedding_by_word2vec(embed_dim, corpus)\n",
    "        word_index = w2v_model.wv.key_to_index\n",
    "        # vocab_size = len(word_index)\n",
    "        \n",
    "        # compute embedding matrix\n",
    "        embedding_matrix = np.zeros((vocab_size, embed_dim))\n",
    "        for word, i in tokenizer.word_index.items():\n",
    "            if w2v_model.wv.__contains__(word):\n",
    "                embedding_matrix[i] = w2v_model.wv.__getitem__(word)\n",
    "    elif model_name == \"glove\":\n",
    "        embed_dim = 200\n",
    "        \n",
    "        # word embedding using GloVe\n",
    "        embeddings_index = word_embedding_by_glove(corpus)\n",
    "        \n",
    "        # compute embedding matrix\n",
    "        embedding_matrix = np.zeros((vocab_size, embed_dim))\n",
    "        for word, i in word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                \n",
    "    print(\"Embedding Matrix Shape:\", embedding_matrix.shape)\n",
    "    return embedding_matrix, tokenizer, embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1a1cc0-4dcf-45a1-bc1d-f732cbda7beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def specificity(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    param:\n",
    "    y_pred - Predicted labels\n",
    "    y_true - True labels \n",
    "    Returns:\n",
    "    Specificity score\n",
    "    \"\"\"\n",
    "    neg_y_true = 1 - y_true\n",
    "    neg_y_pred = 1 - y_pred\n",
    "    fp = K.sum(neg_y_true * y_pred)\n",
    "    tn = K.sum(neg_y_true * neg_y_pred)\n",
    "    specificity = tn / (tn + fp + K.epsilon())\n",
    "    return specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d888a7-5f59-42aa-b3ce-eb08065e13a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_result_plot(history):\n",
    "    spec,  val_spec  = history.history['specificity'], history.history['val_specificity']\n",
    "    loss, val_loss = history.history['loss'], history.history['val_loss']\n",
    "    epochs = range(len(spec))\n",
    "\n",
    "    plt.plot(epochs, spec, 'b', label='Training spec')\n",
    "    plt.plot(epochs, val_spec, 'r', label='Validation spec')\n",
    "    plt.title('Training and validation specificity')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9d129d-ba6a-41c7-a037-15b50b0e4b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neural_network_model(embedding_layer):\n",
    "    # instance de neural network model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # on ajoute toutes les couches nécessaires\n",
    "    model.add(embedding_layer)\n",
    "    model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(16, activation=\"relu\"))\n",
    "    model.add(layers.Dense(16, activation=\"relu\"))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    # on crée les fonctions de loss et d'optimisation\n",
    "    loss = losses.BinaryCrossentropy(from_logits=True)\n",
    "    optimizer = optimizers.Adam(learning_rate=2e-4, epsilon=1e-08)\n",
    "    \n",
    "    # on compile le modèle\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[specificity])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ee560c-6726-49af-b557-485af5d746df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network_model(model, X_train, y_train, X_test, y_test):\n",
    "    # on crée les méthodes de callback permettant d'arrêter l'entraînement dès que certaines conditions sont remplies\n",
    "    callbacks = [\n",
    "        ReduceLROnPlateau(monitor='val_loss', patience=10, cooldown=0),\n",
    "        EarlyStopping(monitor='val_specificity', min_delta=1e-4, patience=10)\n",
    "    ]\n",
    " \n",
    "    history = model.fit(X_train, y_train, batch_size=1024, epochs=20,\n",
    "                        validation_data=(X_test, y_test), callbacks=callbacks,\n",
    "                        verbose=1)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae31a52-0f05-4dc6-8b99-7037fe4a3347",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd2ddc3-6e11-4c0f-8e81-6c81c36309b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/training.1600000.processed.noemoticon.csv\", sep=',',\n",
    "                   encoding=\"ISO-8859-1\", names=[\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad9d78c-5306-48ab-8f47-103b8aaeaa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fc50d3-38aa-4834-8d84-55c0b0ad46c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"target\"].replace(4, 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba72c88d-faa9-4547-83ae-bea711670e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_unique_values(data, \"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc883ec4-c007-4b36-a30b-23e5f1fb111f",
   "metadata": {},
   "source": [
    "Sentiment du tweet 0 pour négatif et 4 pour positif  \n",
    "Dans tous les cas, nous n'avons besoin que du texte du tweet et de la target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74278305-d24c-4c46-838c-ccfd29c49472",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=[\"id\", \"date\", \"flag\", \"user\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5f92b1-1166-4eca-a952-37f3e71819a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dataset is containing\", data.shape[0], \"lines and\", data.shape[1], \"columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2801865c-59ae-46a5-b389-ee8ac26914dc",
   "metadata": {},
   "source": [
    "On commence par prendre un échantillon des données dans un premier temps : 10 000 de chaque target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4dd08e-3804-474c-a259-76a0284f792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = data.groupby(['target']).apply(pd.DataFrame.sample, n=10000, replace=True, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d183c0f8-a419-4f20-a275-1ef20317b92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The sample dataset is containing\", data_words.shape[0], \"lines and\", data_words.shape[1], \"columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b46e0d-5d17-429b-982b-de22ae01dad3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc379d10-e910-4d80-bfd8-fcac046de8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_exclude = [\n",
    "    \"...\", \"..\"\n",
    "]\n",
    "tags_to_remove=[\"NNP\", \"NN\", \"VBG\", \"VBN\", \"CD\"]\n",
    "\n",
    "corpus = natural_language_processing(\n",
    "    data_words, \"text\", stem_or_lemma=\"lemma\", debug=False, word_cloud=True,\n",
    "    tags_to_remove=tags_to_remove, words_to_exclude=words_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfef7c6-a2fa-437e-a424-ca1b4cab8134",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfecb887-d996-4c5a-90c0-8aada2a08bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop des colonnes inutiles\n",
    "data_words.drop(columns=[\"filtpos_text\", \"preproc_text\", \"text\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0262152a-f381-4b17-af61-2f60bc3b8541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_back(text):\n",
    "    return ' '.join(text)\n",
    "\n",
    "data_words['sentence'] = data_words['filtexcl_words'].apply(join_back)\n",
    "data_words.rename(columns={\"filtexcl_words\": \"list_words\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d40bda-bcfc-454b-9725-6165f855d223",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4735cba-2297-4d73-ac1d-0247a24e6d2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Approche modèles sur mesure simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe88420b-d831-4ae6-b08b-860d1ff98c30",
   "metadata": {},
   "source": [
    "Utilisation du bag of words pour le test des modèles sur mesure simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6254e77f-9bec-4d21-8c61-e5c564117e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=800)# vectorizing words and storing in variable X(predictor)\n",
    "X = cv.fit_transform(data_words[\"sentence\"]).toarray()\n",
    "print(X.shape)\n",
    "y = data_words.target.values# y size\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fa9bff-8ffa-4066-975f-8ac20289df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 parties : train 60%, validation 20% and test 20%\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.4,\n",
    "    stratify=y,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size=0.5,\n",
    "    stratify=y_temp,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763ded28-f5c8-413f-9dcb-4c509d594f2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test des 3 Naive Bayes models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2ca839-257d-4973-ac3d-629548489411",
   "metadata": {},
   "outputs": [],
   "source": [
    "for nb_model in [\"gaussian\", \"multinomial\", \"bernoulli\"]:\n",
    "    model = get_naive_bayes_model(nb_model)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    res = display_classification_metrics(model, y_test, y_pred, list_metrics=[\"precision\", \"auc\"], roc_curve=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afe1946-06fe-4e6c-934d-0c4469546795",
   "metadata": {},
   "source": [
    "Bernoulli est meilleur comme model de référence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2bde90-9a4b-47ef-85f3-f837f9e17ec2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test avec Naive Bayes Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55abafc7-dbec-4620-b5fe-bb1494e444d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_naive_bayes_model(\"bernoulli\")\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "res = display_classification_metrics(model, X_val, y_val, y_pred, list_metrics=[\"precision\", \"auc\"], roc_curve=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e893224-b1b4-4094-92a7-6dbc1907cd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "speci_bayes = display_confusion_matrix(actual_class=y_val, predict_class=y_pred, display_labels=[\"Negative\", \"Positive\"], specifity=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df16af4c-80b5-411f-9de0-9854818bdfab",
   "metadata": {},
   "source": [
    "Notre référence sera donc une spécificité à 0.5965."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1b5794-321f-4f4d-a662-69a9ec41159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "truc = bidule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e7993c-7e58-402c-bc90-b01bd27d30cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test avec XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602482a3-3419-4223-a613-89a0677dc0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01a8bba-51fe-4a03-9a65-3a53e1c68373",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = ensemble.GradientBoostingClassifier(random_state=42)\n",
    "# model_xgb = ensemble.GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=42)\n",
    "model_xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = model_xgb.predict(X_val)\n",
    "res_xgb = display_classification_metrics(model_xgb, y_val, y_pred, list_metrics=[\"precision\", \"auc\"], roc_curve=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2f2410-0398-44c6-9fc7-422f1b389b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "speci_xgboost = display_confusion_matrix(actual_class=y_val, predict_class=y_pred_xgb, display_labels=[\"Negative\", \"Positive\"], specifity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78bd45d-f70c-47df-bbfd-02217a8eeb44",
   "metadata": {
    "tags": []
   },
   "source": [
    "### XGBoost GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa78470-be7e-4f34-84b3-615841c83aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model_xgb = ensemble.GradientBoostingClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab6e78e-c532-4a10-9ca1-64aae688e5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 0.1\n",
    "# n_estimators = 100\n",
    "# max_depth = 3\n",
    "\n",
    "# 16 minutes avec ces paramètres\n",
    "parameters = {\n",
    "    \"learning_rate\": [0.05, 0.1, 0.15],\n",
    "    \"max_depth\":[1, 3, 5],\n",
    "    \"n_estimators\":[50, 100, 150]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc36e8aa-8168-4ed4-a763-3f0450c9b0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "scorer = make_scorer(specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89512f7-5009-42ad-80db-0aec01ba39b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "grid_search_xgb = model_selection.GridSearchCV(\n",
    "    estimator=model_xgb,\n",
    "    param_grid=parameters,\n",
    "    # scoring = \"accuracy\",\n",
    "    scoring = scorer,\n",
    "    n_jobs = None,\n",
    "    cv = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f221cf57-391f-493d-9126-1b01aa0a67da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgb_inst=grid_search_xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6367dc-2e65-4d83-84b6-74cd92de4e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb =xgb_inst.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997bbc43-1fd0-422a-a701-27320c36abca",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "delta = round(end - start, 0)\n",
    "print(\"Execution time is:\", delta, \"s, soit\", round(delta/60, 0), \"min, soit\" , round(delta/3600, 0), \"h.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5245fa-2b02-4a66-adb8-d866e1c30d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Les meilleures paramètres sont les suivants :\\n\", grid_search_xgb.best_params_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918d9c1d-3d59-479d-96a1-f4f0a794c2c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b54c77-243f-41cb-b866-3d819d189a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = ensemble.GradientBoostingClassifier(**grid_search_xgb.best_params_)\n",
    "model_xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = model_xgb.predict(X_val)\n",
    "res_xgb = display_classification_metrics(model_xgb, y_val, y_pred, list_metrics=[\"precision\", \"auc\"], roc_curve=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357a5a33-aa6d-4a15-91d5-f97859f51105",
   "metadata": {},
   "outputs": [],
   "source": [
    "speci_xgboost = display_confusion_matrix(actual_class=y_val, predict_class=y_pred_xgb, display_labels=[\"Negative\", \"Positive\"], specifity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb5fca2-5a29-4d38-886c-2666559c6283",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = display_classification_metrics(model_xgb, y_val, y_pred_xgb, all_metrics=True, roc_curve=True, beta=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c328494-f7cb-4ee9-8a75-bb79097e64c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def display_classification_report(y_true, y_pred, display_labels=None):\n",
    "    print(\"Calcul et affichage du rapport de classification\")\n",
    "    clf_report = metrics.classification_report(y_true, y_pred, target_names=display_labels)\n",
    "    print(clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2134b9e4-86bc-421b-8852-d52a7908173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_classification_report(y_val, y_pred_xgb, display_labels=[\"Negative\", \"Positive\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdb5019-b45c-4d1a-a2ca-632e43ddd2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "truc = bidule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22975e08-cd1a-4e72-ba70-5f3d4d079fac",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896e1b7e-e8e3-4717-9420-100238908a23",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd406e20-ada9-4b36-9347-076573978189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e7c0e-8a4c-4bf9-bc1f-e0e16578c619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 parties : train 60%, validation 20% and test 20%\n",
    "X, y = np.array(data_words['sentence']), np.array(data_words['target'])\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = model_selection.train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.4,\n",
    "    stratify=y,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = model_selection.train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size=0.5,\n",
    "    stratify=y_temp,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cd9532-022c-4589-a7e2-52d2a1ef6f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train data shape :\", X_train.shape)\n",
    "print(\"Test data shape :\", X_test.shape)\n",
    "print(\"Validation data shape :\", X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b232c-6e96-40f2-b836-8a2eb50ec3e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1d9c96-3117-49e3-a3f6-f1ed9fa31032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = list of documents\n",
    "corpus = data_words[\"list_words\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cb972a-d393-41ed-823f-0cbb60fbe6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras import layers, losses, optimizers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d4b87f-47f3-4f9e-aabc-cdb385e58d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 280 # max characters in a tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e3b977-0dce-4b91-9d6b-429b5efc3036",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenize_corpus(corpus)\n",
    "    \n",
    "X_train = sequence.pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=MAX_SENTENCE_LENGTH)\n",
    "X_test  = sequence.pad_sequences(tokenizer.texts_to_sequences(X_test) , maxlen=MAX_SENTENCE_LENGTH)\n",
    "X_val  = sequence.pad_sequences(tokenizer.texts_to_sequences(X_val) , maxlen=MAX_SENTENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce29bbd0-5954-4fe7-8c1a-4e17696b65b1",
   "metadata": {},
   "source": [
    "On teste les word embedding Word2Vec et GloVe. Chacun a des paramètres différents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b133a8-6e55-41ba-84c6-fbeba292f833",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dcb176-9f70-4588-97b9-99e93d75f43c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# speci_res = {}\n",
    "\n",
    "# for word_embed_model in [\"word2vec\", \"glove\"]:\n",
    "#     print(\"-\"*70, \"Embedding model\", word_embed_model, \"-\"*70)\n",
    "    \n",
    "#     # word embedding\n",
    "#     embed_mat, tokenizer, embed_dim = compute_corpus_to_word_embedding(corpus, model_name=word_embed_model)\n",
    "#     vocab_size = len(tokenizer.word_index) + 1\n",
    "    \n",
    "#     # create embedding layer\n",
    "#     embedding_layer = layers.Embedding(\n",
    "#         input_dim=vocab_size, output_dim=embed_dim, weights=[embed_mat],\n",
    "#         input_length=MAX_SENTENCE_LENGTH, trainable=False)\n",
    "    \n",
    "#     # create the neural network\n",
    "#     model = create_neural_network_model(embedding_layer)\n",
    "    \n",
    "#     # train the neural network\n",
    "#     history = train_neural_network_model(model, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "#     # show the result into a graph\n",
    "#     display_result_plot(history)\n",
    "    \n",
    "#     # compute confusion matrix and specificity\n",
    "#     y_pred = model.predict(X_val)\n",
    "#     y_pred = np.where(y_pred>=0.5, 1, 0)\n",
    "#     speci = display_confusion_matrix(actual_class=y_val, predict_class=y_pred, display_labels=[\"Negative\", \"Positive\"], specifity=True)\n",
    "#     speci_res[word_embed_model] = speci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5735955-3465-45ed-be25-5564a0895e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speci_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a5f2b5-3dac-4842-9fe4-7ddcad398886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    # word embedding\n",
    "    embed_mat, tokenizer, embed_dim = compute_corpus_to_word_embedding(corpus, model_name=\"glove\")\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    # create embedding layer\n",
    "    embedding_layer = layers.Embedding(\n",
    "    input_dim=vocab_size, output_dim=embed_dim, weights=[embed_mat],\n",
    "    input_length=MAX_SENTENCE_LENGTH, trainable=False)\n",
    "    \n",
    "    # create model\n",
    "    model = keras.Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(16, activation=\"relu\"))\n",
    "    model.add(layers.Dense(16, activation=\"relu\"))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    # on crée les fonctions de loss et d'optimisation\n",
    "    loss = losses.BinaryCrossentropy(from_logits=True)\n",
    "    optimizer = optimizers.Adam(learning_rate=2e-4, epsilon=1e-08)\n",
    "    \n",
    "    # on compile le modèle\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[specificity])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aafeb9-b9b6-4123-baca-28555c3b4fc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# create model\n",
    "model = KerasClassifier(model=create_model, verbose=0)\n",
    "# define the grid search parameters\n",
    "batch_size = [50, 100]\n",
    "epochs = [10, 20]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = model_selection.GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=None, cv=3, error_score='raise')\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea6d352-cb6c-403b-9ff3-aa5972e2473d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6866e27-195b-415f-9155-ed4c85779dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "delta = round(end - start, 0)\n",
    "print(\"Execution time is:\", delta, \"s, soit\", round(delta/60, 0), \"min, soit\" , round(delta/3600, 0), \"h.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2234902-e5ae-41da-bb47-f0a320ac8263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f42518-ab20-400c-bf94-fea99980e278",
   "metadata": {},
   "outputs": [],
   "source": [
    "truc = bidule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fce0238-18c3-40b5-90e2-05eb32676d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fafc3419-5a53-4434-8ef9-c06b312fcb84",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436f4fb4-92f2-4f3f-8058-151a5b0fbc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Dense, LSTM, Conv1D, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0840b2f-2eeb-46d1-8a19-b05e0aba9e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_neural_network_model(embedding_layer):\n",
    "    # on crée la séquence avec toutes les couches de notre réseau de neurones\n",
    "    model = keras.Sequential([\n",
    "        embedding_layer,\n",
    "        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n",
    "        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n",
    "        Conv1D(100, 5, activation='relu'),\n",
    "        GlobalMaxPool1D(),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1, activation='sigmoid'),\n",
    "    ],\n",
    "    name=\"Sentiment_Model\")\n",
    "    \n",
    "    # on crée les fonctions de loss et d'optimisation\n",
    "    loss = losses.BinaryCrossentropy(from_logits=True)\n",
    "    optimizer = optimizers.Adam(learning_rate=2e-4, epsilon=1e-08)\n",
    "    \n",
    "    # on compile le modèle\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[specificity])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c781d12c-9a01-4049-920c-3c489400b952",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "speci_res = {}\n",
    "\n",
    "for word_embed_model in [\"word2vec\", \"glove\"]:\n",
    "    print(\"-\"*70, \"Embedding model\", word_embed_model, \"-\"*70)\n",
    "    \n",
    "    # word embedding\n",
    "    embed_mat, tokenizer, embed_dim = compute_corpus_to_word_embedding(corpus, model_name=word_embed_model)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    # create embedding layer\n",
    "    embedding_layer = layers.Embedding(\n",
    "        input_dim=vocab_size, output_dim=embed_dim, weights=[embed_mat],\n",
    "        input_length=MAX_SENTENCE_LENGTH, trainable=False)\n",
    "    \n",
    "    # create the neural network\n",
    "    model = create_lstm_neural_network_model(embedding_layer)\n",
    "    \n",
    "    # train the neural network\n",
    "    history = train_neural_network_model(model, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # show the result into a graph\n",
    "    display_result_plot(history)\n",
    "    \n",
    "    # compute confusion matrix and specificity\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred = np.where(y_pred>=0.5, 1, 0)\n",
    "    speci = display_confusion_matrix(actual_class=y_val, predict_class=y_pred, display_labels=[\"Negative\", \"Positive\"], specifity=True)\n",
    "    speci_res[word_embed_model] = speci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10054073-e52b-4ad5-a055-717f79142f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "speci_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529bad44-eeb6-4d72-852d-8efbfa55cf4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aefaa4-bd36-4db3-a5a5-cd51e46ee349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00978df8-6056-4e05-92bd-d4cd46c92a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gru_neural_network_model(embedding_layer):\n",
    "    # instance de neural network model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # on ajoute toutes les couches nécessaires\n",
    "    model.add(embedding_layer)\n",
    "    model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "    # model.add(layers.Flatten())\n",
    "    model.add(layers.GRU(64))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(16, activation=\"relu\"))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    # on crée les fonctions de loss et d'optimisation\n",
    "    loss = losses.BinaryCrossentropy(from_logits=False)\n",
    "    optimizer = optimizers.Adam(learning_rate=2e-4, epsilon=1e-08)\n",
    "    \n",
    "    # on compile le modèle\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[specificity])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eb94cd-5d95-4f9c-9afe-d3f8dbda862e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "speci_res = {}\n",
    "\n",
    "for word_embed_model in [\"word2vec\", \"glove\"]:\n",
    "    print(\"-\"*70, \"Embedding model\", word_embed_model, \"-\"*70)\n",
    "    \n",
    "    # word embedding\n",
    "    embed_mat, tokenizer, embed_dim = compute_corpus_to_word_embedding(corpus, model_name=word_embed_model)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    # create embedding layer\n",
    "    embedding_layer = layers.Embedding(\n",
    "        input_dim=vocab_size, output_dim=embed_dim, weights=[embed_mat],\n",
    "        input_length=MAX_SENTENCE_LENGTH, trainable=False)\n",
    "    \n",
    "    # create the neural network\n",
    "    model = create_gru_neural_network_model(embedding_layer)\n",
    "    \n",
    "    # train the neural network\n",
    "    history = train_neural_network_model(model, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # show the result into a graph\n",
    "    display_result_plot(history)\n",
    "    \n",
    "    # compute confusion matrix and specificity\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred = np.where(y_pred>=0.5, 1, 0)\n",
    "    speci = display_confusion_matrix(actual_class=y_val, predict_class=y_pred, display_labels=[\"Negative\", \"Positive\"], specifity=True)\n",
    "    speci_res[word_embed_model] = speci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfb1fdc-2c02-48aa-acd8-d371a50259a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "speci_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68c9dc4-32e8-415f-860b-776e33d01d08",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3527e8-8be4-4666-89ab-c9765f4eeee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9b400d-382f-41b1-ad9a-dc7b6287e5f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a4233a-ad78-4a50-8b32-70eb01f2c253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import *\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig, TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe53dc-203e-4ee5-8091-b74d22ea841b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00855fc-d692-49b0-abe6-5295e1006855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sent in data_words[\"sentence\"]:\n",
    "#     tokens=bert_tokenizer.tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73085f78-a3aa-4854-81cb-624ed800d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=[]\n",
    "attention_masks=[]\n",
    "\n",
    "for sent in data_words[\"sentence\"]:\n",
    "    bert_inp = bert_tokenizer.encode_plus(\n",
    "        sent, add_special_tokens=True, max_length=64, pad_to_max_length=True,\n",
    "        return_attention_mask=True)\n",
    "    input_ids.append(bert_inp['input_ids'])\n",
    "    attention_masks.append(bert_inp['attention_mask'])\n",
    "\n",
    "input_ids=np.asarray(input_ids)\n",
    "attention_masks=np.array(attention_masks)\n",
    "labels=np.array(data_words['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f29d8b-3094-43a9-88ec-c985f3a7ef2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_ids), len(attention_masks), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62cbd24-0cf3-4ce1-9598-30eeb1468a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb837e3-a766-4afd-a636-6b787d03f424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks = [tf.keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=model_save_path,save_weights_only=True,monitor='val_loss',\n",
    "#     mode='min',save_best_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47270299-1a87-4164-adf8-1c93d47e3e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b138d4ab-0bfa-45e9-8e4a-6125693d3925",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = optimizers.Adam(learning_rate=2e-5, epsilon=1e-08)\n",
    "# learning_rate=2e-5 recommended by the Bert team\n",
    "\n",
    "bert_model.compile(loss=loss,optimizer=optimizer,metrics=[tf.keras.metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9a0b6e-de3e-4edb-9c4e-161eb47b2b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e824ae63-6d10-43e5-941e-ebfc31e20fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "history=bert_model.fit(\n",
    "    [train_inp,train_mask],train_label,batch_size=32,epochs=4,\n",
    "    validation_data=([val_inp,val_mask],val_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4473e391-3ca0-4cf9-822d-6bba35ee0932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_result_plot_accuracy(history):\n",
    "    spec,  val_spec  = history.history['categorical_accuracy'], history.history['val_categorical_accuracy']\n",
    "    loss, val_loss = history.history['loss'], history.history['val_loss']\n",
    "    epochs = range(len(spec))\n",
    "\n",
    "    plt.plot(epochs, spec, 'b', label='Training spec')\n",
    "    plt.plot(epochs, val_spec, 'r', label='Validation spec')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afad5d3-4802-461d-afe4-1bb59657f7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_result_plot_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbc453f-01b8-4469-b644-4cd2556f930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute confusion matrix and specificity\n",
    "y_pred = bert_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6530c1b-f5d7-4668-8375-d30fd6840361",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcbeb30-85c1-40ee-9d3e-48dfc9b4f54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = y_pred[\"logits\"].argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdd26d8-ed0a-47ad-bc39-8e11e378df43",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29ef0cf-9a7e-43b6-b162-bfb7cb42ff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_res = []\n",
    "for tup in y_pred[\"logits\"]:\n",
    "    if tup[1]>=0.5:\n",
    "        pred_res.append(1)\n",
    "    else:\n",
    "        pred_res.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eafc9a7-724a-4dd4-b97b-f698c0449358",
   "metadata": {},
   "outputs": [],
   "source": [
    "speci = display_confusion_matrix(actual_class=y_val, predict_class=pred_labels, display_labels=[\"Negative\", \"Positive\"], specifity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e247694-253c-4718-8d1f-8f7f995ac746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env_prj07",
   "language": "python",
   "name": "conda_env_prj07"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
